{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(LogisticNet, self).__init__()\n",
    "        self.linear = nn.Linear(D_in, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lin = self.linear(x)\n",
    "        return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, trX, trY):\n",
    "    x = Variable(trX, requires_grad=False)\n",
    "    y = Variable(trY, requires_grad=False)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid(model, loss_func, valX, valY):\n",
    "    x = Variable(valX, requires_grad=False)\n",
    "    y = Variable(valY, requires_grad=False)\n",
    "\n",
    "    outputs = model(x)\n",
    "    val_loss = loss_func(outputs, y)\n",
    "    # calculate accuracy\n",
    "    _, predY = torch.max(outputs.data, 1)\n",
    "    correct = (predY == y.data).sum()\n",
    "    val_acc = float(correct) / y.size(0)\n",
    "    return val_loss.data[0], val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:9.123 val acc:0.106\n",
      "val loss:8.922 val acc:0.106\n",
      "val loss:8.652 val acc:0.119\n",
      "val loss:8.338 val acc:0.122\n",
      "val loss:8.005 val acc:0.131\n",
      "val loss:7.672 val acc:0.136\n",
      "val loss:7.353 val acc:0.131\n",
      "val loss:7.054 val acc:0.122\n",
      "val loss:6.777 val acc:0.119\n",
      "val loss:6.518 val acc:0.108\n",
      "val loss:6.278 val acc:0.111\n",
      "val loss:6.054 val acc:0.111\n",
      "val loss:5.844 val acc:0.111\n",
      "val loss:5.649 val acc:0.106\n",
      "val loss:5.466 val acc:0.097\n",
      "val loss:5.293 val acc:0.103\n",
      "val loss:5.128 val acc:0.111\n",
      "val loss:4.971 val acc:0.117\n",
      "val loss:4.820 val acc:0.128\n",
      "val loss:4.673 val acc:0.128\n",
      "val loss:4.531 val acc:0.136\n",
      "val loss:4.392 val acc:0.139\n",
      "val loss:4.256 val acc:0.144\n",
      "val loss:4.122 val acc:0.150\n",
      "val loss:3.991 val acc:0.164\n",
      "val loss:3.863 val acc:0.169\n",
      "val loss:3.737 val acc:0.172\n",
      "val loss:3.613 val acc:0.189\n",
      "val loss:3.492 val acc:0.197\n",
      "val loss:3.374 val acc:0.203\n",
      "val loss:3.260 val acc:0.217\n",
      "val loss:3.150 val acc:0.219\n",
      "val loss:3.045 val acc:0.211\n",
      "val loss:2.946 val acc:0.217\n",
      "val loss:2.852 val acc:0.225\n",
      "val loss:2.766 val acc:0.236\n",
      "val loss:2.687 val acc:0.239\n",
      "val loss:2.614 val acc:0.250\n",
      "val loss:2.548 val acc:0.253\n",
      "val loss:2.488 val acc:0.256\n",
      "val loss:2.433 val acc:0.261\n",
      "val loss:2.382 val acc:0.278\n",
      "val loss:2.335 val acc:0.283\n",
      "val loss:2.292 val acc:0.283\n",
      "val loss:2.251 val acc:0.286\n",
      "val loss:2.213 val acc:0.294\n",
      "val loss:2.177 val acc:0.308\n",
      "val loss:2.144 val acc:0.317\n",
      "val loss:2.112 val acc:0.328\n",
      "val loss:2.082 val acc:0.325\n",
      "val loss:2.053 val acc:0.333\n",
      "val loss:2.025 val acc:0.342\n",
      "val loss:1.998 val acc:0.344\n",
      "val loss:1.971 val acc:0.347\n",
      "val loss:1.944 val acc:0.353\n",
      "val loss:1.917 val acc:0.367\n",
      "val loss:1.891 val acc:0.378\n",
      "val loss:1.864 val acc:0.383\n",
      "val loss:1.837 val acc:0.389\n",
      "val loss:1.810 val acc:0.400\n",
      "val loss:1.783 val acc:0.408\n",
      "val loss:1.756 val acc:0.422\n",
      "val loss:1.730 val acc:0.428\n",
      "val loss:1.703 val acc:0.450\n",
      "val loss:1.677 val acc:0.458\n",
      "val loss:1.651 val acc:0.469\n",
      "val loss:1.626 val acc:0.475\n",
      "val loss:1.601 val acc:0.483\n",
      "val loss:1.576 val acc:0.492\n",
      "val loss:1.552 val acc:0.494\n",
      "val loss:1.529 val acc:0.511\n",
      "val loss:1.506 val acc:0.522\n",
      "val loss:1.484 val acc:0.531\n",
      "val loss:1.463 val acc:0.533\n",
      "val loss:1.442 val acc:0.542\n",
      "val loss:1.422 val acc:0.553\n",
      "val loss:1.402 val acc:0.556\n",
      "val loss:1.383 val acc:0.561\n",
      "val loss:1.364 val acc:0.567\n",
      "val loss:1.347 val acc:0.572\n",
      "val loss:1.329 val acc:0.569\n",
      "val loss:1.313 val acc:0.583\n",
      "val loss:1.297 val acc:0.589\n",
      "val loss:1.281 val acc:0.600\n",
      "val loss:1.266 val acc:0.608\n",
      "val loss:1.251 val acc:0.608\n",
      "val loss:1.237 val acc:0.614\n",
      "val loss:1.223 val acc:0.617\n",
      "val loss:1.210 val acc:0.625\n",
      "val loss:1.197 val acc:0.636\n",
      "val loss:1.185 val acc:0.639\n",
      "val loss:1.172 val acc:0.639\n",
      "val loss:1.161 val acc:0.639\n",
      "val loss:1.149 val acc:0.639\n",
      "val loss:1.138 val acc:0.644\n",
      "val loss:1.127 val acc:0.642\n",
      "val loss:1.116 val acc:0.642\n",
      "val loss:1.106 val acc:0.644\n",
      "val loss:1.096 val acc:0.647\n",
      "val loss:1.086 val acc:0.653\n",
      "val loss:1.076 val acc:0.664\n",
      "val loss:1.066 val acc:0.675\n",
      "val loss:1.057 val acc:0.678\n",
      "val loss:1.048 val acc:0.683\n",
      "val loss:1.039 val acc:0.681\n",
      "val loss:1.030 val acc:0.681\n",
      "val loss:1.021 val acc:0.686\n",
      "val loss:1.013 val acc:0.697\n",
      "val loss:1.004 val acc:0.700\n",
      "val loss:0.996 val acc:0.700\n",
      "val loss:0.988 val acc:0.703\n",
      "val loss:0.980 val acc:0.706\n",
      "val loss:0.972 val acc:0.706\n",
      "val loss:0.964 val acc:0.706\n",
      "val loss:0.956 val acc:0.711\n",
      "val loss:0.949 val acc:0.711\n",
      "val loss:0.941 val acc:0.711\n",
      "val loss:0.934 val acc:0.714\n",
      "val loss:0.927 val acc:0.714\n",
      "val loss:0.920 val acc:0.722\n",
      "val loss:0.913 val acc:0.725\n",
      "val loss:0.906 val acc:0.725\n",
      "val loss:0.899 val acc:0.728\n",
      "val loss:0.893 val acc:0.731\n",
      "val loss:0.887 val acc:0.731\n",
      "val loss:0.880 val acc:0.731\n",
      "val loss:0.874 val acc:0.731\n",
      "val loss:0.868 val acc:0.731\n",
      "val loss:0.862 val acc:0.731\n",
      "val loss:0.856 val acc:0.733\n",
      "val loss:0.850 val acc:0.739\n",
      "val loss:0.845 val acc:0.742\n",
      "val loss:0.839 val acc:0.747\n",
      "val loss:0.834 val acc:0.753\n",
      "val loss:0.829 val acc:0.758\n",
      "val loss:0.823 val acc:0.758\n",
      "val loss:0.818 val acc:0.758\n",
      "val loss:0.813 val acc:0.761\n",
      "val loss:0.808 val acc:0.761\n",
      "val loss:0.803 val acc:0.764\n",
      "val loss:0.798 val acc:0.764\n",
      "val loss:0.794 val acc:0.767\n",
      "val loss:0.789 val acc:0.767\n",
      "val loss:0.785 val acc:0.772\n",
      "val loss:0.780 val acc:0.781\n",
      "val loss:0.776 val acc:0.781\n",
      "val loss:0.771 val acc:0.783\n",
      "val loss:0.767 val acc:0.783\n",
      "val loss:0.763 val acc:0.783\n",
      "val loss:0.758 val acc:0.783\n",
      "val loss:0.754 val acc:0.783\n",
      "val loss:0.750 val acc:0.786\n",
      "val loss:0.746 val acc:0.786\n",
      "val loss:0.742 val acc:0.786\n",
      "val loss:0.738 val acc:0.786\n",
      "val loss:0.734 val acc:0.786\n",
      "val loss:0.731 val acc:0.792\n",
      "val loss:0.727 val acc:0.792\n",
      "val loss:0.723 val acc:0.792\n",
      "val loss:0.720 val acc:0.792\n",
      "val loss:0.716 val acc:0.792\n",
      "val loss:0.712 val acc:0.792\n",
      "val loss:0.709 val acc:0.797\n",
      "val loss:0.705 val acc:0.797\n",
      "val loss:0.702 val acc:0.800\n",
      "val loss:0.699 val acc:0.803\n",
      "val loss:0.695 val acc:0.803\n",
      "val loss:0.692 val acc:0.803\n",
      "val loss:0.689 val acc:0.803\n",
      "val loss:0.685 val acc:0.803\n",
      "val loss:0.682 val acc:0.806\n",
      "val loss:0.679 val acc:0.806\n",
      "val loss:0.676 val acc:0.806\n",
      "val loss:0.673 val acc:0.806\n",
      "val loss:0.670 val acc:0.808\n",
      "val loss:0.667 val acc:0.808\n",
      "val loss:0.664 val acc:0.808\n",
      "val loss:0.661 val acc:0.808\n",
      "val loss:0.658 val acc:0.808\n",
      "val loss:0.655 val acc:0.808\n",
      "val loss:0.652 val acc:0.808\n",
      "val loss:0.650 val acc:0.808\n",
      "val loss:0.647 val acc:0.808\n",
      "val loss:0.644 val acc:0.811\n",
      "val loss:0.642 val acc:0.811\n",
      "val loss:0.639 val acc:0.814\n",
      "val loss:0.636 val acc:0.817\n",
      "val loss:0.634 val acc:0.822\n",
      "val loss:0.631 val acc:0.822\n",
      "val loss:0.628 val acc:0.822\n",
      "val loss:0.626 val acc:0.822\n",
      "val loss:0.623 val acc:0.822\n",
      "val loss:0.621 val acc:0.822\n",
      "val loss:0.619 val acc:0.825\n",
      "val loss:0.616 val acc:0.825\n",
      "val loss:0.614 val acc:0.825\n",
      "val loss:0.611 val acc:0.828\n",
      "val loss:0.609 val acc:0.828\n",
      "val loss:0.607 val acc:0.828\n",
      "val loss:0.604 val acc:0.828\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "data = digits['data']\n",
    "target = digits['target']\n",
    "# separate data\n",
    "trX, teX, trY, teY = train_test_split(data, target, test_size=0.2, random_state=0)\n",
    "\n",
    "n_samples = trX.shape[0]\n",
    "input_dim = trX.shape[1]\n",
    "n_classes = 10\n",
    "model = LogisticNet(input_dim, n_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "trX = torch.from_numpy(trX).float()\n",
    "teX = torch.from_numpy(teX).float()\n",
    "trY = torch.from_numpy(trY.astype(np.int64))\n",
    "teY = torch.from_numpy(teY.astype(np.int64))\n",
    "\n",
    "N_EPOCHS = 200\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss = train(model, loss_func, optimizer, trX, trY)\n",
    "    val_loss, val_acc = valid(model, loss_func, teX, teY)\n",
    "    print 'val loss:%.3f val acc:%.3f' % (val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
