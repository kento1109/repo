{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(LogisticNet, self).__init__()\n",
    "        self.linear = nn.Linear(D_in, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lin = self.linear(x)\n",
    "        return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_func, optimizer, trX, trY):\n",
    "    x = Variable(trX, requires_grad=False)\n",
    "    y = Variable(trY, requires_grad=False)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_func(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x_val):\n",
    "    output = model.forward(x_val)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid(model, loss_func, valX, valY):\n",
    "    x = Variable(valX, requires_grad=False)\n",
    "    y = Variable(valY, requires_grad=False)\n",
    "\n",
    "    outputs = model(x)\n",
    "    val_loss = loss_func(outputs, y)\n",
    "    # calculate accuracy\n",
    "    _, predY = torch.max(outputs.data, 1)\n",
    "    correct = (predY == y.data).sum()\n",
    "    val_acc = float(correct) / y.size(0)\n",
    "    return val_loss.data[0], val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss:6.147 val acc:0.078\n",
      "val loss:6.078 val acc:0.078\n",
      "val loss:5.981 val acc:0.081\n",
      "val loss:5.862 val acc:0.081\n",
      "val loss:5.724 val acc:0.081\n",
      "val loss:5.574 val acc:0.081\n",
      "val loss:5.415 val acc:0.086\n",
      "val loss:5.252 val acc:0.083\n",
      "val loss:5.088 val acc:0.086\n",
      "val loss:4.928 val acc:0.083\n",
      "val loss:4.774 val acc:0.081\n",
      "val loss:4.630 val acc:0.081\n",
      "val loss:4.498 val acc:0.072\n",
      "val loss:4.380 val acc:0.072\n",
      "val loss:4.274 val acc:0.083\n",
      "val loss:4.183 val acc:0.086\n",
      "val loss:4.104 val acc:0.094\n",
      "val loss:4.036 val acc:0.089\n",
      "val loss:3.976 val acc:0.106\n",
      "val loss:3.923 val acc:0.114\n",
      "val loss:3.875 val acc:0.122\n",
      "val loss:3.828 val acc:0.128\n",
      "val loss:3.783 val acc:0.128\n",
      "val loss:3.737 val acc:0.136\n",
      "val loss:3.689 val acc:0.136\n",
      "val loss:3.639 val acc:0.144\n",
      "val loss:3.587 val acc:0.147\n",
      "val loss:3.531 val acc:0.158\n",
      "val loss:3.474 val acc:0.186\n",
      "val loss:3.414 val acc:0.197\n",
      "val loss:3.352 val acc:0.208\n",
      "val loss:3.289 val acc:0.211\n",
      "val loss:3.225 val acc:0.217\n",
      "val loss:3.161 val acc:0.222\n",
      "val loss:3.097 val acc:0.228\n",
      "val loss:3.034 val acc:0.231\n",
      "val loss:2.973 val acc:0.250\n",
      "val loss:2.913 val acc:0.258\n",
      "val loss:2.855 val acc:0.269\n",
      "val loss:2.800 val acc:0.286\n",
      "val loss:2.747 val acc:0.283\n",
      "val loss:2.695 val acc:0.289\n",
      "val loss:2.647 val acc:0.294\n",
      "val loss:2.599 val acc:0.303\n",
      "val loss:2.554 val acc:0.306\n",
      "val loss:2.510 val acc:0.311\n",
      "val loss:2.468 val acc:0.319\n",
      "val loss:2.427 val acc:0.328\n",
      "val loss:2.387 val acc:0.333\n",
      "val loss:2.347 val acc:0.342\n",
      "val loss:2.309 val acc:0.350\n",
      "val loss:2.271 val acc:0.361\n",
      "val loss:2.234 val acc:0.372\n",
      "val loss:2.197 val acc:0.375\n",
      "val loss:2.162 val acc:0.375\n",
      "val loss:2.127 val acc:0.383\n",
      "val loss:2.093 val acc:0.389\n",
      "val loss:2.060 val acc:0.392\n",
      "val loss:2.028 val acc:0.406\n",
      "val loss:1.996 val acc:0.408\n",
      "val loss:1.966 val acc:0.414\n",
      "val loss:1.936 val acc:0.425\n",
      "val loss:1.907 val acc:0.428\n",
      "val loss:1.879 val acc:0.436\n",
      "val loss:1.852 val acc:0.444\n",
      "val loss:1.825 val acc:0.447\n",
      "val loss:1.800 val acc:0.456\n",
      "val loss:1.774 val acc:0.461\n",
      "val loss:1.750 val acc:0.461\n",
      "val loss:1.726 val acc:0.467\n",
      "val loss:1.703 val acc:0.472\n",
      "val loss:1.680 val acc:0.475\n",
      "val loss:1.658 val acc:0.489\n",
      "val loss:1.636 val acc:0.497\n",
      "val loss:1.615 val acc:0.506\n",
      "val loss:1.594 val acc:0.508\n",
      "val loss:1.574 val acc:0.514\n",
      "val loss:1.554 val acc:0.519\n",
      "val loss:1.535 val acc:0.525\n",
      "val loss:1.516 val acc:0.528\n",
      "val loss:1.498 val acc:0.533\n",
      "val loss:1.479 val acc:0.542\n",
      "val loss:1.462 val acc:0.547\n",
      "val loss:1.444 val acc:0.556\n",
      "val loss:1.427 val acc:0.564\n",
      "val loss:1.411 val acc:0.567\n",
      "val loss:1.394 val acc:0.567\n",
      "val loss:1.378 val acc:0.569\n",
      "val loss:1.363 val acc:0.569\n",
      "val loss:1.347 val acc:0.572\n",
      "val loss:1.332 val acc:0.586\n",
      "val loss:1.318 val acc:0.592\n",
      "val loss:1.303 val acc:0.594\n",
      "val loss:1.289 val acc:0.594\n",
      "val loss:1.276 val acc:0.597\n",
      "val loss:1.262 val acc:0.597\n",
      "val loss:1.249 val acc:0.597\n",
      "val loss:1.236 val acc:0.600\n",
      "val loss:1.223 val acc:0.603\n",
      "val loss:1.211 val acc:0.608\n",
      "val loss:1.199 val acc:0.611\n",
      "val loss:1.187 val acc:0.611\n",
      "val loss:1.175 val acc:0.614\n",
      "val loss:1.164 val acc:0.617\n",
      "val loss:1.152 val acc:0.625\n",
      "val loss:1.141 val acc:0.633\n",
      "val loss:1.131 val acc:0.639\n",
      "val loss:1.120 val acc:0.639\n",
      "val loss:1.110 val acc:0.639\n",
      "val loss:1.099 val acc:0.644\n",
      "val loss:1.089 val acc:0.647\n",
      "val loss:1.080 val acc:0.658\n",
      "val loss:1.070 val acc:0.661\n",
      "val loss:1.060 val acc:0.667\n",
      "val loss:1.051 val acc:0.669\n",
      "val loss:1.042 val acc:0.672\n",
      "val loss:1.033 val acc:0.672\n",
      "val loss:1.024 val acc:0.678\n",
      "val loss:1.016 val acc:0.678\n",
      "val loss:1.007 val acc:0.678\n",
      "val loss:0.999 val acc:0.681\n",
      "val loss:0.991 val acc:0.683\n",
      "val loss:0.983 val acc:0.686\n",
      "val loss:0.975 val acc:0.689\n",
      "val loss:0.967 val acc:0.689\n",
      "val loss:0.960 val acc:0.689\n",
      "val loss:0.952 val acc:0.689\n",
      "val loss:0.945 val acc:0.692\n",
      "val loss:0.937 val acc:0.694\n",
      "val loss:0.930 val acc:0.694\n",
      "val loss:0.923 val acc:0.694\n",
      "val loss:0.916 val acc:0.697\n",
      "val loss:0.910 val acc:0.706\n",
      "val loss:0.903 val acc:0.708\n",
      "val loss:0.896 val acc:0.711\n",
      "val loss:0.890 val acc:0.717\n",
      "val loss:0.884 val acc:0.717\n",
      "val loss:0.877 val acc:0.717\n",
      "val loss:0.871 val acc:0.722\n",
      "val loss:0.865 val acc:0.725\n",
      "val loss:0.859 val acc:0.725\n",
      "val loss:0.853 val acc:0.731\n",
      "val loss:0.848 val acc:0.733\n",
      "val loss:0.842 val acc:0.739\n",
      "val loss:0.836 val acc:0.739\n",
      "val loss:0.831 val acc:0.739\n",
      "val loss:0.826 val acc:0.739\n",
      "val loss:0.820 val acc:0.739\n",
      "val loss:0.815 val acc:0.739\n",
      "val loss:0.810 val acc:0.742\n",
      "val loss:0.805 val acc:0.744\n",
      "val loss:0.800 val acc:0.744\n",
      "val loss:0.795 val acc:0.750\n",
      "val loss:0.790 val acc:0.750\n",
      "val loss:0.785 val acc:0.753\n",
      "val loss:0.780 val acc:0.753\n",
      "val loss:0.776 val acc:0.756\n",
      "val loss:0.771 val acc:0.761\n",
      "val loss:0.766 val acc:0.767\n",
      "val loss:0.762 val acc:0.769\n",
      "val loss:0.758 val acc:0.769\n",
      "val loss:0.753 val acc:0.769\n",
      "val loss:0.749 val acc:0.769\n",
      "val loss:0.745 val acc:0.769\n",
      "val loss:0.741 val acc:0.781\n",
      "val loss:0.736 val acc:0.783\n",
      "val loss:0.732 val acc:0.783\n",
      "val loss:0.728 val acc:0.783\n",
      "val loss:0.724 val acc:0.783\n",
      "val loss:0.720 val acc:0.783\n",
      "val loss:0.717 val acc:0.783\n",
      "val loss:0.713 val acc:0.789\n",
      "val loss:0.709 val acc:0.789\n",
      "val loss:0.705 val acc:0.789\n",
      "val loss:0.702 val acc:0.789\n",
      "val loss:0.698 val acc:0.792\n",
      "val loss:0.695 val acc:0.792\n",
      "val loss:0.691 val acc:0.792\n",
      "val loss:0.688 val acc:0.792\n",
      "val loss:0.684 val acc:0.792\n",
      "val loss:0.681 val acc:0.792\n",
      "val loss:0.677 val acc:0.792\n",
      "val loss:0.674 val acc:0.792\n",
      "val loss:0.671 val acc:0.792\n",
      "val loss:0.668 val acc:0.792\n",
      "val loss:0.664 val acc:0.794\n",
      "val loss:0.661 val acc:0.794\n",
      "val loss:0.658 val acc:0.800\n",
      "val loss:0.655 val acc:0.800\n",
      "val loss:0.652 val acc:0.803\n",
      "val loss:0.649 val acc:0.803\n",
      "val loss:0.646 val acc:0.806\n",
      "val loss:0.643 val acc:0.806\n",
      "val loss:0.640 val acc:0.808\n",
      "val loss:0.637 val acc:0.808\n",
      "val loss:0.634 val acc:0.808\n",
      "val loss:0.632 val acc:0.808\n",
      "val loss:0.629 val acc:0.808\n",
      "val loss:0.626 val acc:0.811\n",
      "val loss:0.623 val acc:0.811\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "data = digits['data']\n",
    "target = digits['target']\n",
    "# separate data\n",
    "trX, teX, trY, teY = train_test_split(data, target, test_size=0.2, random_state=0)\n",
    "\n",
    "n_samples = trX.shape[0]\n",
    "input_dim = trX.shape[1]\n",
    "n_classes = 10\n",
    "model = LogisticNet(input_dim, n_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "trX = torch.from_numpy(trX).float()\n",
    "teX = torch.from_numpy(teX).float()\n",
    "trY = torch.from_numpy(trY.astype(np.int64))\n",
    "teY = torch.from_numpy(teY.astype(np.int64))\n",
    "\n",
    "N_EPOCHS = 300\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    loss = train(model, loss_func, optimizer, trX, trY)\n",
    "    val_loss, val_acc = valid(model, loss_func, teX, teY)\n",
    "    print 'val loss:%.3f val acc:%.3f' % (val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
